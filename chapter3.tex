% !TEX root = thesis.tex

\todo{Here we shall summarize what this chapter is about}

\section{General idea}

Diagrammatic Monte Carlo (from here on abbreviated DMC) is a numerical method developed by Prokof?ev et al. \cite{MishchenkoA.2000DqMC} in order to calculate quantities $ Q(\{ y \}) $, given in terms of a series of integrals with an ever increasing number of integration variables

\begin{equation}
	\label{eq:integralSeries}
	Q(\{ y \})
	= \sum_{n=0}^\infty \sum_{\xi_n} \int \diff x_1 \dots \diff x_n \, D_n(\xi_n, \, \{y\}, \, x_1, \, \dots , \, x_n) \,.
\end{equation}

Here $ \{y\} $ is a set of external variables and $ \xi_n $ indexes the different terms of order $ n $ which are given by the function $ D_n $. Terms corresponding to $ n = 0 $ are understood as being known functions of the external variables. In case of a discrete internal variable $ x_i $, the corresponding integral is exchanged in favor of a sum.

The method is based on the Metropolis-Hastings algorithm which samples terms of the integral series in the $ (\xi_n, \, \{y\}, \, x_1, \, \dots , \, x_n) $ parameter space. That is, a random walk is preformed between the different integrands in the integral series, as well as between the possible values of the integration variables. In this random walk it is also permissible to include one or more of the external variables. Since the terms at $ n = 0 $ are known functions it is then possible to calculate the value of $ Q $ by keeping track of the frequency by which these terms are sampled, as well as the length of the sequence of sampled terms.

To clarify any ambiguities, the DMC method will next be demonstrated by a few simple examples.


%Including external variables in the random walk will increased the parameter space so that more statistics is needed to reach the same level of certainty as having them fixed. Not to surprisingly, it therefore becomes a trade off between having more degrees of freedom in $ Q $ but then having to spend more time computing.



\subsection{Example 1}

In this example the task is to compute $ 1 + C $, where for convenience it is assumed that $ C > 1 $ is a constant. The integral series (\ref{eq:integralSeries}) will then consist out of two constant zeroth order terms only, making the parameter space discrete with the two possibilities $ \xi_0 = 0 $ and $ \xi_0 = 1 $ corresponding to the terms $ 1 $ and $ C $ respectively. Hence this is probably the simplest possible implementation of DMC and will make for a good first encounter.

To follow the recipe of the Metropolis-Hastings algorithm, the function $ f $ is defined as being nothing but $ D_0(\xi_0) $ from (\ref{eq:integralSeries}). By this definition $ f $ becomes proportional to the probability density function 

\begin{equation}
	\rho(x) = \frac{f(x)}{\sum_y f(y)} = \frac{f(x)}{1 + C}
	\; ; \quad
	x \in \{1\,, C \} \,,
\end{equation}
from whose probability distribution function $ P $ the sequence of $ \xi_0 $'s will be drawn. Further more there can be at most four types of transitions between the two states in parameter space. By introducing the corresponding transition probabilities: $ P(1|C) $, $ P(C|1) $, $ P(1|1) $ and $ P(C|C) $, the Markov process \question{(is this the correct word?)} to be simulated may then be illustrated as in figure (\ref{fig:example1}) below.

\begin{figure}[H]
	\centering
 	\includegraphics[width=\textwidth, ]{{"Images/Examples/First example"}.pdf}
	\caption{\todo{Caption this plz!}}
	\label{fig:example1}
\end{figure}

To simplify matters, the proposal distributions $ W(1|C) $, $ W(1|1) $, $ W(C|1) $ and $ W(C|C) $ are all chosen to have the same probability of $ 1/2 $. Then, according to (\ref{eq:acceptanceRatio}), the acceptance-rejection ratios will be given by $ A(1|1) = A(1|C) = A(C|C) = 1 $ and $ A(C|1) = 1/C $. By keeping track of how many times $ \xi_0 = 0 $ appears in the generated sequence, lets say $ N_0 $ times, as well as the the total length $ N $ of the sequence, it is possible to calculate $ 1 + C $. This since
\begin{equation}
	\label{eq:example1Prop}
	N_0 \propto \rho(1) \propto 1
	\; , \quad
	N - N_0 \propto \rho(C) \propto C
	\quad \Rightarrow \quad
	\frac{N_0}{N - N_0} = \frac{1}{C}
\end{equation}
so that
\begin{equation}
	\label{eq:example1Answ}
	1 + C = 1 + \frac{N - N_0}{N_0} \,.
\end{equation}

\todo{Include an implementation written in python?}

\subsection{Example 2}

In this second example the task is to compute the sum $ 1 + \int_a^b x \diff x $. Instead of having of having two zeroth order terms as in the first example, there is now a zeroth and first order term. This implies that the parameter space is no longer completely discrete with merely two possible states. Similarly to the first example, the zeroth order term is still represented in parameter space by the state $ \xi_0 = 0 $. On the other hand, the first order term corresponds to the infinite number of states $ (\xi_1 = 0, \, x \in [a, \, b]) $. This notation of the states in parameter space is a little cumbersome, however, by assuming $ b > a > 1 $ it is possible to create a one-to-one mapping which maps $ \xi_0 = 0 $ and $ (\xi_1 = 0, \, x \in [a, \, b]) $ onto $ x = 1 $ and $ x \in [a, \, b] $ respectively. Using this notation, a function $ f $ is defined by
\begin{equation}
	f(x) =
	\begin{dcases}
		D_0(\xi_0 = 0) = 1 \; , \quad &x = 1 \\
		D_1(\xi_1 = 0, x) = x \; , \quad &x \in [a, \, b] \,,
   	\end{dcases}
\end{equation}
and thus becomes proportional to the probability density
\begin{equation}
	\rho(x)
	= \frac{f(x)}{\sum_y \rho(y)}
	= \frac{f(x)}{1 + \int_a^b x \diff x}
	\; ; \quad
	x \in \{1\} \cup [a, \, b] \,.
\end{equation}

In order to cover the whole parameter space, it is more than sufficient to consider four types of transitions having the transition probabilities $ P(1|1) $, $ P(1|x) $, $ P(x|1) $ and $ P(x|x') $. Here it is understood that $ x, x' \in [a, \, b] $ and thus corresponds to a state of the first order term. This Markov process might be illustrated as figure \ref{fig:example2} below.

\begin{figure}[H]
	\centering
 	\includegraphics[width=\textwidth, ]{{"Images/Examples/Second example"}.pdf}
	\caption{\todo{Caption this plz!}}
	\label{fig:example2}
\end{figure}

In order to construct proposal distributions to or from the first order term, it is convenient to introduce the collection of states referred to as $ \smallint $, in which all states $ x \in [a, \, b] $. Using this notation, the proposal distributions are chosen to be
\begin{equation}
	\begin{split}
		W(1|1) &= \tfrac{1}{2} \\
		W(x|1) &= W(\smallint | 1) = \tfrac{1}{2}
	\end{split}
	\quad \quad
	\begin{split}
		W(1|x) &= W(1 | \smallint) \, U(x) = \tfrac{1}{2} \tfrac{1}{b-a} \\
		W(x|x') &= W(\smallint | \smallint) \,U(x') = \tfrac{1}{2} \tfrac{1}{b-a} \,.
	\end{split}
\end{equation}
The quantities $ W(1|1) $, $ W(1|\smallint) $, $ W(\smallint | 1) $ and $ W(\smallint | \smallint) $ are simply the probability of proposing a state corresponding to a certain term in the integral series and have been set equally probable. The quantity $ U(x) $ is the uniform probability distribution on the interval $ [a, \, b] $ which $ x $ is sampled from. Then, from the proposal distributions follow the acceptance rations
\begin{equation}
	\begin{split}
		A(1|1) &= 1 \\
		A(x|1) &= \text{min} \left( 1, \,  \tfrac{1}{x[b - a]} \right) \\
	\end{split}
	\quad \quad
	\begin{split}
		A(1|x) &= \text{min} \left( 1, \, x[b - a] \right) \\
		A(x|x') &= \text{min} \left( 1, \, \tfrac{x'}{x} \right) \,.
	\end{split}
\end{equation}

By keeping track of the $ N_0 $ number of times $ x = 1 $ appears in the sequence of states and also the length $ N $ of the sequence, it follows from (\ref{eq:example1Prop}) and (\ref{eq:example1Answ}) that
\begin{equation}
	1 + \int_a^b x \diff x = 1 + \frac{N - N_0}{N_0} \,.
\end{equation}

\subsection{Example 3}

In this third and final example, the task is to compute something which is more similar to the actual problem. Hence, the sum to compute is of the form,
\begin{equation}
	\Sigma = 1 + \int_{a}^{b} \diff x \, e^{-x} + \int_{a'}^{b'} \diff y \int_{a''}^{b''} \diff z \, e^{-y -z} \,,
\end{equation}
and consists of a zeroth, first and second order term. By assuming that $ b > a > 1 $, a similar one-to-one mapping of the states in parameter space as used in the previous example may also be used here. Again the zeroth and first order terms are chosen to be represented by $ x = 1 $ and $ x \in [a, \, b] $ respectively, whilst the second order term is to be represented by $ x = (y, z) \in [a', \, b'] \times [a'', \, b''] $. In terms of this parameter $ x $, the function $ f $ is defined as
\begin{equation}
	f(x) =
	\begin{dcases}
		D_0(\xi_0 = 0) = 1 \; , \quad &x = 1 \\
		D_1(\xi_1 = 0, x) = e^{-x} \; , \quad &x \in [a, \, b] \\
		D_2(\xi_2 = 0, x) = e^{-y - z} \; , \quad &x \in [a', \, b'] \times [a'', \, b''] \,.
   	\end{dcases}
\end{equation}
Normalizing this function the following probability density function is obtained,
\begin{equation}
	\rho(x) = \frac{f(x)}{\Sigma}
	\; ; \quad
	x \in \{1\} \cup [a, \, b] \cup [a', \, b'] \times [a'', \, b'']\,.
\end{equation}





\section{Polaron implementation}

\todo{semi definite integrands (?)}


\todo{Add to $ \hat H $ the chemical potential. Our energies are to be independent of $ \mu $. mu is simply a tool to prevent the}

\todo{Here we will demonstrate what happens when we are using $ T = 0 $ and not having any free electrons in the system at the beginning. (wtf!?)}

\subsection{Update function 1}
\subsection{Update function 2}
\subsection{Update function 3}

\section{Sample self energy}

\subsection{Divergent diagram}

The first order proper self energy diagram is proportional to $ 1/\sqrt{\tau} $ for small $ \tau $ and thus diverges when $ \tau \rightarrow 0 $. This combined with a discretized histogram becomes a problem for the bins in the neighborhood of $ \tau = 0 $ since they acquire more mass then they are supposed to. In order to fix this we could use a much finer discretization or calculate the value for these bins somehow else.

The value of the diagram for $ \vec p = \vec 0 $ and $ \Delta G = 0 $ is

\begin{equation}
	S^{(1)}(\tau) = \frac{\alpha}{\sqrt{\pi \tau}} e^{(\mu - \omega)\tau}
\end{equation}

The value of the bins in our histogram is

\begin{equation}
	\tilde V_i \propto \frac{1}{\Delta \tau} \int_{\tau_i}^{\tau_{i + 1}} S^{(1)} (t) \diff t
	= \frac{\alpha}{\Delta \tau \sqrt{\omega - \mu}} \left[ \text{erf}\left(\sqrt{\omega - \mu} \sqrt{\tau_{i+1}}\right) - \text{erf}\left(\sqrt{\omega - \mu} \sqrt{\tau_i}\right) \right]
\end{equation}

 but the true value should be
 
\begin{equation}
	V_i \propto S^{(1)} \left( \tfrac{\tau_{i+1} + \tau_i}{2} \right)	
\end{equation}

For all bins which fulfil $ \tilde V_i  - V_i > \epsilon $ we instead of using DMC utilise ordinary stochastic integration using MC for the fixed times $ (\tau_{i+1} + \tau_i)/2 $.

\section{Boldification}

