% !TEX root = ../thesis.tex

\todo{Here we shall summarize what this chapter is about}

\section{General idea}

\todo{In thermal equilibrium}

Diagrammatic Monte Carlo (from here on abbreviated DMC) is a numerical method developed by Prokof'ev et al. \cite{MishchenkoA.2000DqMC} in order to calculate quantities $ Q(\{ y \}) $, given in terms of a series of integrals with an ever increasing number of integration variables
\begin{equation}
	\label{eq:integralSeries}
	Q(\{ y \})
	= \sum_{n=0}^\infty \sum_{\xi_n} \int \diff x_1 \dots \diff x_n \, D_n(\xi_n, \, \{y\}, \, x_1, \, \dots , \, x_n) \,.
\end{equation}
Here $ \{y\} $ is a set of external variables and $ \xi_n $ indexes the different terms of order $ n $ which are given by the function $ D_n $. Terms corresponding to $ n = 0 $ are understood as being known functions of the external variables. In case of a discrete internal variable $ x_i $, the corresponding integral is exchanged in favor of a sum.

The method is based on the Metropolis-Hastings algorithm which samples terms of the integral series in the $ (\xi_n, \, \{y\}, \, x_1, \, \dots , \, x_n) $ parameter space. That is, a random walk is preformed between the different integrands in the integral series, as well as between the possible values of the integration variables. In this random walk it is also permissible to include one or more of the external variables. Since the terms at $ n = 0 $ are known functions it is then possible to calculate the value of $ Q $ by keeping track of the frequency by which these terms are sampled, as well as the length of the sequence of sampled terms.

To clarify any ambiguities, the DMC method will next be demonstrated by a few simple examples.

\subsection{Example 1}

In this example the task is to compute $ 1 + C $, where for convenience it is assumed that $ C > 1 $ is a constant. The integral series (\ref{eq:integralSeries}) will then consist out of two constant zeroth order terms only, making the parameter space discrete with the two possibilities $ \xi_0 = 0 $ and $ \xi_0 = 1 $ corresponding to the terms $ 1 $ and $ C $ respectively. Hence this is probably the simplest possible implementation of DMC and will make for a good first encounter.

\todo{The function $ D_n $ here play the role of $ f $ in the previous section about the MH algorithm.}

To follow the recipe of the Metropolis-Hastings algorithm, the function $ f $ is defined as being nothing but $ D_0(\xi_0) $ from (\ref{eq:integralSeries}). By this definition $ f $ becomes proportional to the probability density function 

\begin{equation}
	\rho(x) = \frac{f(x)}{\sum_y f(y)} = \frac{f(x)}{1 + C}
	\; ; \quad
	x \in \{1\,, C \} \,,
\end{equation}
from whose probability distribution function $ P $ the sequence of $ \xi_0 $'s will be drawn. Further more there can be at most four types of transitions between the two states in parameter space. By introducing the corresponding transition probabilities: $ P(1|C) $, $ P(C|1) $, $ P(1|1) $ and $ P(C|C) $, the Markov process \question{(is this the correct word?)} to be simulated may then be illustrated as in figure (\ref{fig:example1}) below.

\begin{figure}[H]
	\centering
 	\includegraphics[width=\textwidth, ]{{"Images/Examples/First example"}.pdf}
	\caption{\todo{Caption this plz!}}
	\label{fig:example1}
\end{figure}

To simplify matters, the proposal distributions $ W(1|C) $, $ W(1|1) $, $ W(C|1) $ and $ W(C|C) $ are all chosen to have the same probability of $ 1/2 $. Then, according to (\ref{eq:acceptanceRatio}), the acceptance-rejection ratios will be given by $ A(1|1) = A(1|C) = A(C|C) = 1 $ and $ A(C|1) = 1/C $. By keeping track of how many times $ \xi_0 = 0 $ appears in the generated sequence, lets say $ N_0 $ times, as well as the the total length $ N $ of the sequence, it is possible to calculate $ 1 + C $. This since
\begin{equation}
	\label{eq:example1Prop}
	N_0 \propto \rho(1) \propto 1
	\; , \quad
	N - N_0 \propto \rho(C) \propto C
	\quad \Rightarrow \quad
	\frac{N_0}{N - N_0} = \frac{1}{C}
\end{equation}
so that
\begin{equation}
	\label{eq:example1Answ}
	1 + C = 1 + \frac{N - N_0}{N_0} \,.
\end{equation}

\todo{Include an implementation written in python?}

\subsection{Example 2}

In this second example the task is to compute the sum $ 1 + \int_a^b x \diff x $.

\todo{the only unknown here is the value of the integral. The $ 1 $ is solely used for normalization purposes.}

Instead of having of having two zeroth order terms as in the first example, there is now a zeroth and first order term. This implies that the parameter space is no longer completely discrete with merely two possible states. Similarly to the first example, the zeroth order term is still represented in parameter space by the state $ \xi_0 = 0 $. On the other hand, the first order term corresponds to the infinite number of states $ (\xi_1 = 0, \, x \in [a, \, b]) $. This notation of the states in parameter space is a little cumbersome, however, by assuming $ b > a > 1 $ it is possible to create a one-to-one mapping which maps $ \xi_0 = 0 $ and $ (\xi_1 = 0, \, x \in [a, \, b]) $ onto $ x = 1 $ and $ x \in [a, \, b] $ respectively. Using this notation, a function $ f $ is defined by
\begin{equation}
	f(x) =
	\begin{dcases}
		D_0(\xi_0 = 0) = 1 \; , \quad &x = 1 \\
		D_1(\xi_1 = 0, x) = x \; , \quad &x \in [a, \, b] \,,
   	\end{dcases}
\end{equation}
and thus becomes proportional to the probability density
\begin{equation}
	\rho(x)
	= \frac{f(x)}{\sum_y \rho(y)}
	= \frac{f(x)}{1 + \int_a^b x \diff x}
	\; ; \quad
	x \in \{1\} \cup [a, \, b] \,.
\end{equation}

In order to cover the whole parameter space, it is more than sufficient to consider four types of transitions having the transition probabilities $ P(1|1) $, $ P(1|x) $, $ P(x|1) $ and $ P(x|x') $. Here it is understood that $ x, x' \in [a, \, b] $ and thus corresponds to a state of the first order term. This Markov process might be illustrated as figure \ref{fig:example2} below.

\begin{figure}[H]
	\centering
 	\includegraphics[width=\textwidth, ]{{"Images/Examples/Second example"}.pdf}
	\caption{\todo{Caption this plz!}}
	\label{fig:example2}
\end{figure}

In order to construct proposal distributions to or from the first order term, it is convenient to introduce the collection of states referred to as $ \smallint $, in which all states $ x \in [a, \, b] $. Using this notation, the proposal distributions are chosen to be
\begin{equation}
	\begin{split}
		W(1|1) &= \tfrac{1}{2} \\
		W(x|1) &= W(\smallint | 1) = \tfrac{1}{2}
	\end{split}
	\quad \quad
	\begin{split}
		W(1|x) &= W(1 | \smallint) \, U_{a,b}(x) = \tfrac{1}{2} \tfrac{1}{b-a} \\
		W(x|x') &= W(\smallint | \smallint) \,U_{a,b}(x') = \tfrac{1}{2} \tfrac{1}{b-a} \,.
	\end{split}
\end{equation}
The quantities $ W(1|1) $, $ W(1|\smallint) $, $ W(\smallint | 1) $ and $ W(\smallint | \smallint) $ are simply the probability of proposing a state corresponding to a certain term in the integral series and have been set equally probable. The quantity $ U_{a,b}(x) = [b - a]^{-1} $ is the uniform probability distribution on the interval $ [a, \, b] $ from which $ x $ is sampled. Then, from the proposal distributions follow the acceptance rations
\begin{equation}
	\begin{split}
		A(1|1) &= 1 \\
		A(x|1) &= \text{min} \Big( 1, \,  [b - a]^{-1} \, x^{-1} \Big) \\
	\end{split}
	\quad \quad
	\begin{split}
		A(1|x) &= \text{min} \Big( 1, \,  [b - a] \, x \Big) \\
		A(x|x') &= \text{min} \Big( 1, \, x^{-1} \, x' \Big) \,.
	\end{split}
\end{equation}

By keeping track of the $ N_0 $ number of times $ x = 1 $ appears in the sequence of states and also the length $ N $ of the sequence, it follows from (\ref{eq:example1Prop}) and (\ref{eq:example1Answ}) that
\begin{equation}
	1 + \int_a^b x \diff x = 1 + \frac{N - N_0}{N_0} \,.
\end{equation}

\subsection{Example 3}

In this third and final example, the task is to compute something which is more similar to the actual problem. Hence, the sum to compute is of the form,
\begin{equation}
	1 + \int_{a}^{b} \diff x \, e^{-x} + \int_{a'}^{b'} \diff y \int_{a''}^{b''} \diff z \, e^{-y -z} \,,
\end{equation}
and consists of a zeroth, first and second order term. By assuming that $ b > a > 1 $, a similar one-to-one mapping of the states in parameter space as used in the previous example may also be used here. Again the zeroth and first order terms are chosen to be represented by $ x = 1 $ and $ x \in [a, \, b] $ respectively, whilst the second order term is to be represented by $ x = (y, z) \in [a', \, b'] \times [a'', \, b''] $. In terms of this parameter $ x $, the function $ f $ is defined as
\begin{equation}
	f(x) =
	\begin{dcases}
		D_0(\xi_0 = 0) = 1 \; , \quad &x = 1 \\
		D_1(\xi_1 = 0, x) = e^{-x} \; , \quad &x \in [a, \, b] \\
		D_2(\xi_2 = 0, x) = e^{-y - z} \; , \quad &x \in [a', \, b'] \times [a'', \, b''] \,.
   	\end{dcases}
\end{equation}
By normalization a probability density function $ \rho $ is obtained whose probability distribution $ P $ and seven types of transitions form a Markov process illustrated in figure \ref{fig:example3}.

\begin{figure}[H]
	\centering
 	\includegraphics[width=\textwidth, ]{{"Images/Examples/Third example"}.pdf}
	\caption{\todo{Caption this plz!}}
	\label{fig:example3}
\end{figure}

In order to construct transition probabilities it is convenient to introduce the collection of states $ \smallint $ and $ \smallint \!\! \smallint $. These contain all states in parameter space corresponding the first and second order term respectively. The transition probabilities are then chosen as
\begin{equation}
	\begin{split}
		W(1|x) &= W(1|\smallint) \, U_{a,b}(x) \\
		W(x|1) &= W(\smallint|1) \\
		W(y,z|x) &= W(\smallint \!\! \smallint | \smallint) \, U_{a,b}(x) \\
		W(x|x') &= W(\smallint | \smallint) \,U_{a,b}(x') \,,
	\end{split}
	\quad \quad
	\begin{split}
		W(x|y,z) &= W(\smallint | \smallint \!\! \smallint) \, U_{a',b'}(y) \, U_{a'',b''}(z) \\
		W(y,z|y',z) &= W(\smallint \!\! \smallint | \smallint \!\! \smallint) \, W(\smallint \!\! \smallint | y') \, U_{a',b'}(y') \\
		W(y,z|y,z') &= W(\smallint \!\! \smallint | \smallint \!\! \smallint) \, \, W(\smallint \!\! \smallint | z') \, U_{a'',b''}(z') \\
		\;
	\end{split}
\end{equation}
where
\begin{equation}
	\begin{split}
		W(1|\smallint) &= 1 \\
		W(\smallint | 1) &= W(\smallint | \smallint) = W(\smallint | \smallint \!\! \smallint) = \tfrac{1}{3} \\
		W(\smallint \!\! \smallint | \smallint) &= W(\smallint \!\! \smallint | \smallint \!\! \smallint) = W(\smallint \!\! \smallint | y') = W(\smallint \!\! \smallint | z') = \tfrac{1}{2} \,.
	\end{split}
\end{equation}
The discrete proposal distributions $ W(\smallint \!\! \smallint | y') $ and $ W(\smallint \!\! \smallint | z') $ correspond to choosing to update either the $ y $ or the $ z $ part of the state $ x $. Having all of this information it is then trivial to calculate the acceptance ratios, which become
\begin{equation}
	\begin{split}
		W(1|x) &= \text{min} \Big(1, \, \tfrac{1}{3} [b - a] \, e^{-x} \Big) \\
		W(x|1) &= \text{min} \Big(1, \, 3 \, [b - a]^{-1} \, e^{x} \Big) \\
		W(x|x') &= \text{min} \Big(1, \, e^{x - x'} \Big) \\
		W(x|y,z) &=\text{min} \Big(1, \, \tfrac{2}{3} [b' - a'][b'' - a''][b - a]^{-1} \, e^{x - y - z} \Big) \\
		W(y,z|x) &= \text{min} \Big(1, \, \tfrac{3}{2} [b' - a']^{-1}[b'' - a'']^{-1}[b - a] \, e^{y + z - x} \Big) \\
		W(y,z|y',z) &= \text{min} \Big(1, \, e^{y - y'} \Big) \\
		W(y,z|y,z') &= \text{min} \Big(1, \, e^{z - z'} \Big) \,.
	\end{split}
\end{equation}

By defining $ N_0 $ and $ N $ in accordance to what was done in the previous example, the value of the sum is obtained by
\begin{equation}
	1 + \int_{a}^{b} \diff x \, e^{-x} + \int_{a'}^{b'} \diff y \int_{a''}^{b''} \diff z \, e^{-y -z}
	= 1 + \frac{N - N_0}{N_0} \,.
\end{equation}

\section{Polaron implementation \question{Propagator sampling?}}

The task at hand is now to implement the DMC method in order to calculate the electronic single-particle Green's function $ \Gt(\alpha, \mu, \vec p, \tau) $. The integral series (\ref{eq:integralSeries}) will then be nothing but the diagrammatic series (\ref{eq:GinTermsOfDiagrams}), with the external parameters $ \{ y \} = \{ \alpha, \mu, \vec p, \tau \} $. The integration variables $ x_i $'s must therefore correspond to the internal imaginary-times and momenta, where the internal momenta are chosen to be those of the phonon propagators. By then representing the internal momenta in spherical coordinates $ \vec q = (q \sin \theta \cos \varphi, q \sin \theta \sin \varphi, q \cos \theta) $, the momentum integrals should be replaced according to
\begin{equation}
	\int \frac{\diff^3 q}{(2 \pi)^3}
	\rightarrow
	\int \limits_{q=0}^\infty \int \limits_{\theta=0}^{\pi} \int \limits_{\varphi=0}^{2\pi} \frac{q^2 \sin \theta \diff q \diff \theta \diff \varphi}{(2 \pi)^3} \,.
\end{equation}
The beauty of this is realized when recalling that each phonon brings with it a factor $ V(q)^2 \propto q^{-2} $, whose momentum dependence exactly cancel with that of the integral. For example, the expression corresponding the first order diagram then becomes
\begin{equation}
	-
	\int \limits_{0}^{\tau} \diff \tau_2
	\int \limits_{0}^{\tau_2} \diff \tau_1
	\int \limits_0^\infty \diff q
	\int \limits_0^\pi \diff \theta
	\int \limits_0^{2 \pi} \diff \varphi
	\, \Gt_0(\vec p, \tau_1)
	\Gt_0(\vec p - \vec q, \tau_2 - \tau_1)
	\tilde \Dt_0(\vec q, \tau_2 - \tau_1)
	\Gt_0(\vec p, \tau - \tau_2) \,,
\end{equation}
where $ \tilde \Dt_0(\vec q, \tau) $ is a phonon propagator which has absorbed the factor $ q^2 \sin \theta  \, (2\pi)^{-3} $ originating from the integral, the interaction potential $ V(q)^2 $ along with the factor $ -1 $ from the Feynman rules. Thus
\begin{equation}
	\tilde \Dt_0(\vec q, \tau)
	=
	\frac{2\sqrt 2 \pi \alpha}{(2\pi)^3} \sin \theta \, \exp \{ - \tau\}
\end{equation}
which is a quantity always larger than or equal to zero. The integrand of each and every integral in the integral series will be entirely made up out of $ \Gt_0 $'s and $ \tilde \Dt_0 $'s which implies that every contribution to $ \Gt $ is a positive contribution. This will simplify the DMC implementation since it wont be necessary to calculate what sign each sampled parameter space state comes with.

The external parameters $ \vec p $ and $ \tau $ will be the only ones allowed to change during the Markov process. Including $ \alpha $ and/or $ \mu $ would cause the parameter space to increase and the statistics to be spread out. Thus more computation time would be needed to calculate quantities to the same level of certainty as having $ \alpha $ and $ \mu $ fixed.

The value of the propagator $ \Gt $ will be computed at the discrete set of points $ \vec p_i = \Delta p(0.5 + i) \, \hat{ \vec e}_z $ and $ \tau_j = \Delta \tau (0.5 + j) $ where $ i = 0, \, 1, \, 2, \, \dots, \, N - 1  $, $ j = 0, \, 1, \, 2, \, \dots, \, M - 1 $ and the resolution is chosen in terms of $ \Delta p $ and $ \Delta \tau $. To calculate $ \Gt(\vec p_i, \tau_j) = \Gt_{i, j} $, both a bin $ N_0 $ and a two dimensional histogram $ N $ are used, even though $ \vec p $ and $ \tau $ will be continuous variables. Thus the bin $ N_{i,j} $ of the histogram will be covering the range $ p \in [p_i - \Delta p/2, \, p_i + \Delta p/2] $ and $ \tau \in [\tau_j - \Delta \tau/2, \, \tau_j + \Delta \tau/2] $ of the external parameter values. The value of every bin in the histogram along with $ N_0 $ are initially put to zero. Then, each time a parameter space state is sampled, it will be checked wether or not this state belongs to the zeroth order term $ \Gt_0(\vec p, \tau) $. If this is the case, the value of $ N_0 $ is increased by 1, if not, the bin in the histogram according to the external parameters is increased by 1. By doing this, the quantities $ N_0 $ and $ N_{i,j} $ can be shown to be proportional to
\begin{equation}
	\begin{split}
		N_0
		&\propto
		\int \limits_{0}^{N\Delta p} \! \! \diff p \int \limits_{0}^{M\Delta p} \! \! \diff \tau \, \Gt_0(\vec p, \tau) \\
		&=
		\Delta p \, \Delta \tau \sum_{k,l} \Gt_0(\vec p_k, \tau_l) + \mathcal{O} \left( [\Delta p]^3 + [\Delta \tau]^3 \right)
	\end{split}
\end{equation}
and
\begin{equation}
	\begin{split}
		N_{i,j}
		&\propto
		\int \limits_{p_{i - 0.5}}^{p_{i + 0.5}} \! \! \! \diff p \int \limits_{\tau_{j - 0.5}}^{\tau_{j + 0.5}} \! \! \! \diff \tau \left[ \Gt(\vec p, \tau) - \Gt_0(\vec p, \tau) \right] \\
		&=
		\Delta p \, \Delta \tau \left[ \Gt(\vec p_i, \tau_j) - \Gt_0(\vec p_i, \tau_j) \right] + \mathcal{O} \left( [\Delta p]^3 + [\Delta \tau]^3 \right)
	\end{split}
\end{equation}
respectively. Omitting the errors due to discretization, the interacting electronic propagator is then found to be
\begin{equation}
	\Gt(\vec p_i, \tau_j)
	=
	\Gt_0(\vec p_i, \tau_j) + N_{i, j} \frac{\sum_{k, l} \Gt_0(\vec p_k, \tau_l)}{N_0} \,.
\end{equation}

What remains to be discussed is how the sampling of states in parameter space occurs. As the name Diagrammatic Monte Carlo suggest, the diagrammatic representation of the integral series will be used. That is, the functions $ D_n $ in the integral series (\ref{eq:integralSeries}) is to be though of as a Feynman diagram (without the integrals). The random walk, which is used to simulate the Markov process, will thus be in terms of such diagrams. It is then important that each and every such diagram must be possible to reach in order to cover all of parameter space and thus maintaining ergodicity. To make sure that this actually is the case, a set of update procedures \cite{MishchenkoA.2000DqMC} have been constructed, which is the topic to follow.



\subsection*{Change of diagram length in time, type 1}

\begin{figure}[H]
	\begin{fmffile}{NORMALchangeOfDiagramLengthType1}
		\begin{equation*}
		        	\begin{gathered}
				\begin{fmfgraph*}(40, 7)
					\fmfstraight
					\fmftop{t1,t2,t3,t4,t5,t6}
					\fmfbottom{b1,b2,b3,b4,b5,b6}
					\fmf{dots}{b1,b2}
					\fmf{fermion}{b2,b4}
					\fmf{fermion}{b4,b6}
					\fmf{dashes}{b2,t2}
					\fmf{dashes}{b4,t4}
					\fmfdot{b2,b4}
					\fmfv{label=$ \tau_{2n - 2} $, label.angle=-90}{b2}
					\fmfv{label=$ \tau_{2n - 1} $, label.angle=-90}{b4}
        					\fmfv{label=$ \textcolor{highlight}{\tau} $, label.angle=0}{b6}
				\end{fmfgraph*}
        			\end{gathered}
			\quad \quad \rightarrow \quad
		        	\begin{gathered}
				\begin{fmfgraph*}(40, 7)
					\fmfstraight
					\fmftop{t1,t2,t3,t4,t5,t6}
					\fmfbottom{b1,b2,b3,b4,b5,b6}
					\fmf{dots}{b1,b2}
					\fmf{fermion}{b2,b4}
					\fmf{fermion}{b4,b6}
					\fmf{dashes}{b2,t2}
					\fmf{dashes}{b4,t4}
					\fmfdot{b2,b4}
					\fmfv{label=$ \tau_{2n - 2} $, label.angle=-90}{b2}
					\fmfv{label=$ \tau_{2n - 1} $, label.angle=-90}{b4}
        					\fmfv{label=$ \textcolor{highlight}{\tau'} $, label.angle=0}{b6}
				\end{fmfgraph*}
        			\end{gathered}
		\end{equation*}
	\end{fmffile}
	\caption{\todo{A caption.}}
	\label{fig:NORMALcodl1}
\end{figure}






\subsection*{Change of diagram length in time, type 2}



\begin{figure}[H]
	\begin{fmffile}{NORMALchangeOfDiagramLengthType2}
		\begin{equation*}
		        	\begin{gathered}
				\begin{fmfgraph*}(50, 7)
					\fmfstraight
					\fmftop{t1,t2,t3,t4,t5,t6,t7,t8,t9}
					\fmfbottom{b1,b2,b3,b4,b5,b6,b7,b8,b9}
					\fmf{dots}{b1,b2}
					\fmf{fermion}{b2,b4}
					\fmf{fermion}{b4,b6}
					\fmf{dots}{b6,b7}
					\fmf{fermion}{b7,b9}
					\fmf{dashes}{b2,t2}
					\fmf{dashes}{b4,t4}
					\fmf{dashes}{b6,t6}
					\fmf{dashes}{b7,t7}
					\fmfdot{b2,b4,b6,b7}
					\fmfv{label=$ \tau_{k -1} $, label.angle=-90}{b2}
					\fmfv{label=$ \textcolor{highlight}{ \tau_{k} } $, label.angle=-90}{b4}
					\fmfv{label=$ \textcolor{highlight}{ \tau_{k + 1} \;\;\; } $, label.angle=-90}{b6}
					\fmfv{label=$ \;\;\;\;\; \textcolor{highlight}{ \tau_{2n - 1} } $, label.angle=-90}{b7}
        					\fmfv{label=$ \textcolor{highlight}{\tau} $, label.angle=0}{b9}
				\end{fmfgraph*}
        			\end{gathered}
			\quad \quad \rightarrow \quad
		        	\begin{gathered}
				\begin{fmfgraph*}(50, 7)
					\fmfstraight
					\fmftop{t1,t2,t3,t4,t5,t6,t7,t8,t9}
					\fmfbottom{b1,b2,b3,b4,b5,b6,b7,b8,b9}
					\fmf{dots}{b1,b2}
					\fmf{fermion}{b2,b4}
					\fmf{fermion}{b4,b6}
					\fmf{dots}{b6,b7}
					\fmf{fermion}{b7,b9}
					\fmf{dashes}{b2,t2}
					\fmf{dashes}{b4,t4}
					\fmf{dashes}{b6,t6}
					\fmf{dashes}{b7,t7}
					\fmfdot{b2,b4,b6,b7}
					\fmfv{label=$ \tau_{k -1} $, label.angle=-90}{b2}
					\fmfv{label=$ \textcolor{highlight}{ \tau_{k}' } $, label.angle=-90}{b4}
					\fmfv{label=$ \textcolor{highlight}{ \tau_{k + 1}' \;\;\; } $, label.angle=-90}{b6}
					\fmfv{label=$ \;\;\;\;\; \textcolor{highlight}{ \tau_{2n - 1}' } $, label.angle=-90}{b7}
        					\fmfv{label=$ \textcolor{highlight}{\tau'} $, label.angle=0}{b9}
				\end{fmfgraph*}
        			\end{gathered}
		\end{equation*}
	\end{fmffile}
	\caption{\todo{A caption.}}
	\label{fig:NORMALcodl2}
\end{figure}



\subsection*{Change of external momentum}
\todo{Isotropic problem $ \rightarrow $ direction independence. Do we keep the direction constant?}

\begin{figure}[H]
	\begin{fmffile}{NORMALchangeOfExternalMomentum}
		\begin{equation*}
		        	\begin{gathered}
				\begin{fmfgraph*}(40, 7)
					\fmfstraight
					\fmftop{t1,t2,t3,t4,t5,t6,t7,t8}
					\fmfbottom{b1,b2,b3,b4,b5,b6,b7,b8}
					\fmf{fermion, label=$ \textcolor{highlight}{\vec p} $, label.side=left}{b1,b3}
					\fmf{fermion, label=$ \textcolor{highlight}{\vec p_0} $, label.side=left}{b3,b5}
					\fmf{dots}{b5,b6}
					\fmf{fermion, label=$ \textcolor{highlight}{\vec p} $, label.side=left}{b6,b8}
					\fmf{dashes}{b3,t3}
					\fmf{dashes}{b5,t5}
					\fmf{dashes}{b6,t6}
					\fmfdot{b3,b5,b6}
				\end{fmfgraph*}
        			\end{gathered}
			\quad \quad \rightarrow \quad \quad
		        	\begin{gathered}
				\begin{fmfgraph*}(40, 7)
					\fmfstraight
					\fmftop{t1,t2,t3,t4,t5,t6,t7,t8}
					\fmfbottom{b1,b2,b3,b4,b5,b6,b7,b8}
					\fmf{fermion, label=$ \textcolor{highlight}{\vec p'} $, label.side=left}{b1,b3}
					\fmf{fermion, label=$ \textcolor{highlight}{\vec p_0'} $, label.side=left}{b3,b5}
					\fmf{dots}{b5,b6}
					\fmf{fermion, label=$ \textcolor{highlight}{\vec p'} $, label.side=left}{b6,b8}
					\fmf{dashes}{b3,t3}
					\fmf{dashes}{b5,t5}
					\fmf{dashes}{b6,t6}
					\fmfdot{b3,b5,b6}
				\end{fmfgraph*}
        			\end{gathered}
		\end{equation*}
	\end{fmffile}
	\caption{\todo{A caption.}}
	\label{fig:NORMALcoem}
\end{figure}

\subsection*{Change of transferred momentum magnitude}
\todo{Momentum transferred via a phonon propagator}

\begin{figure}[H]
	\begin{fmffile}{NORMALchangeOfTransferredMomentum}
		\begin{equation*}
		        	\begin{gathered}
				\begin{fmfgraph*}(40, 20)
					\fmfstraight
					\fmftop{t1,t2,t3,t4,t5,t6,t7,t8}
					\fmfbottom{b1,b2,b3,b4,b5,b6,b7,b8}
					\fmf{dots}{b1,b2}
					\fmf{fermion, label=$ \textcolor{highlight}{\vec p_k} $, label.side=left}{b2,b4}
					\fmf{dots}{b4,b5}
					\fmf{fermion, label=$ \textcolor{highlight}{\vec p_l} $, label.side=left}{b5,b7}
					\fmf{dots}{b7,b8}
					\fmf{dashes, tension=1.5}{b4,v1}
					\fmf{phantom}{v1,t4}
					\fmf{dashes, tension=1.5}{b5,v2}
					\fmf{phantom}{v2,t5}
					\fmf{dashes, left, tension=0, label=$ \textcolor{highlight}{\vec q_k} $, label.side=left}{b2,b7}
					\fmfdot{b2,b4,b5,b7}
				\end{fmfgraph*}
        			\end{gathered}
			\quad \quad \rightarrow \quad \quad
		        	\begin{gathered}
				\begin{fmfgraph*}(40, 20)
					\fmfstraight
					\fmftop{t1,t2,t3,t4,t5,t6,t7,t8}
					\fmfbottom{b1,b2,b3,b4,b5,b6,b7,b8}
					\fmf{dots}{b1,b2}
					\fmf{fermion, label=$ \textcolor{highlight}{\vec p_k'} $, label.side=left}{b2,b4}
					\fmf{dots}{b4,b5}
					\fmf{fermion, label=$ \textcolor{highlight}{\vec p_l'} $, label.side=left}{b5,b7}
					\fmf{dots}{b7,b8}
					\fmf{dashes, tension=1.5}{b4,v1}
					\fmf{phantom}{v1,t4}
					\fmf{dashes, tension=1.5}{b5,v2}
					\fmf{phantom}{v2,t5}
					\fmf{dashes, left, tension=0, label=$ \textcolor{highlight}{\vec q_k'} $, label.side=left}{b2,b7}
					\fmfdot{b2,b4,b5,b7}
				\end{fmfgraph*}
        			\end{gathered}
		\end{equation*}
	\end{fmffile}
	\caption{\todo{A caption.}}
	\label{fig:NORMALcotm}
\end{figure}

\subsection*{Change of transferred momentum direction}

\subsection*{Vertex shift in time}

\begin{figure}[H]
	\begin{fmffile}{NORMALvertexShiftInTime}
		\begin{equation*}
		        	\begin{gathered}
				\begin{fmfgraph*}(40, 7)
					\fmfstraight
					\fmftop{t1,t2,t3,t4,t5,t6,t7}
					\fmfbottom{b1,b2,b3,b4,b5,b6,b7}
					\fmf{dots}{b1,b2}
					\fmf{fermion}{b2,b4}
					\fmf{fermion}{b4,b6}
					\fmf{dots}{b6,b7}
					\fmf{dashes}{b2,t2}
					\fmf{dashes}{b4,t4}
					\fmf{dashes}{b6,t6}
					\fmfdot{b2,b4,b6}
					\fmfv{label=$ \tau_{k -1} $, label.angle=-90}{b2}
					\fmfv{label=$ \textcolor{highlight}{ \tau_{k} } $, label.angle=-90}{b4}
					\fmfv{label=$ \tau_{k + 1} $, label.angle=-90}{b6}
				\end{fmfgraph*}
        			\end{gathered}
			\quad \quad \rightarrow \quad \quad
		        	\begin{gathered}
				\begin{fmfgraph*}(40, 7)
					\fmfstraight
					\fmftop{t1,t2,t3,t4,t5,t6,t7}
					\fmfbottom{b1,b2,b3,b4,b5,b6,b7}
					\fmf{dots}{b1,b2}
					\fmf{fermion}{b2,b4}
					\fmf{fermion}{b4,b6}
					\fmf{dots}{b6,b7}
					\fmf{dashes}{b2,t2}
					\fmf{dashes}{b4,t4}
					\fmf{dashes}{b6,t6}
					\fmfdot{b2,b4,b6}
					\fmfv{label=$ \tau_{k -1} $, label.angle=-90}{b2}
					\fmfv{label=$ \textcolor{highlight}{ \tau_{k}' } $, label.angle=-90}{b4}
					\fmfv{label=$ \tau_{k + 1} $, label.angle=-90}{b6}
				\end{fmfgraph*}
        			\end{gathered}
		\end{equation*}
	\end{fmffile}
	\caption{\todo{A caption.}}
	\label{fig:NORMALvsit}
\end{figure}

\subsection*{Change of diagram structure}

\subsection*{Change of digram order}






\section{Self energy sampling}

\subsection{Divergent diagram}

The first order proper self energy diagram is proportional to $ 1/\sqrt{\tau} $ for small $ \tau $ and thus diverges when $ \tau \rightarrow 0 $. This combined with a discretized histogram becomes a problem for the bins in the neighborhood of $ \tau = 0 $ since they acquire more mass then they are supposed to. In order to fix this we could use a much finer discretization or calculate the value for these bins somehow else.

The value of the diagram for $ \vec p = \vec 0 $ and $ \Delta G = 0 $ is

\begin{equation}
	S^{(1)}(\tau) = \frac{\alpha}{\sqrt{\pi \tau}} e^{(\mu - \omega)\tau}
\end{equation}

The value of the bins in our histogram is

\begin{equation}
	\tilde V_i \propto \frac{1}{\Delta \tau} \int_{\tau_i}^{\tau_{i + 1}} S^{(1)} (t) \diff t
	= \frac{\alpha}{\Delta \tau \sqrt{\omega - \mu}} \left[ \text{erf}\left(\sqrt{\omega - \mu} \sqrt{\tau_{i+1}}\right) - \text{erf}\left(\sqrt{\omega - \mu} \sqrt{\tau_i}\right) \right]
\end{equation}

 but the true value should be
 
\begin{equation}
	V_i \propto S^{(1)} \left( \tfrac{\tau_{i+1} + \tau_i}{2} \right)	
\end{equation}

For all bins which fulfil $ \tilde V_i  - V_i > \epsilon $ we instead of using DMC utilise ordinary stochastic integration using MC for the fixed times $ (\tau_{i+1} + \tau_i)/2 $.

\section{Boldification}

